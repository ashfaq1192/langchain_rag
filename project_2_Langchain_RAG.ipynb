{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIkq7t+Wt0o3/gdjDmJgmK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashfaq1192/langchain_rag/blob/main/project_2_Langchain_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2: Langchain RAG Project"
      ],
      "metadata": {
        "id": "_mHjZ5gAdRxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***RAG***"
      ],
      "metadata": {
        "id": "D-js6q_oVZ9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieved Augmented Generation (RAG) is a system or tool used for storing the embedded data which is used by LLM when we ask a domain specific or technical question from the LLM. Our prompt first goes to the RAG Vector Database and collects the required embedded data and then goes towards the LLM which give use the specific required output in concise form with the help of given data through RAG alongwith its own previous knowledge which was given at the time of training of LLLM. Because, LLMs are trained till specific date, so for example, if we want the LLM to give us the details or required information from the list of projects we have done which definitely LLM does not know. So,for that purpose, we have to store our all projects in RAG Vector Database and then we will ask the question from the LLM which will first go to the Vector Database and then will go towards LLM and then LLM will be such in a position that it can answer our questions efficiently.\n",
        "\n",
        "1. RAG is used for Searching data from Vector Database. It is like a bag in which we have everthing about the specific domain and we direct the LLM to use that bag before answering so that our answer can be free from hallucinations and the LLM gives us the answer after searching from the RAG.\n",
        "2. The reason behind is that we cannot train the LLM each time for the small tiny tasks, so we just give access to RAG Vector Database which stores data in embedded form."
      ],
      "metadata": {
        "id": "FP8-jAP8Vil4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding**"
      ],
      "metadata": {
        "id": "xpRjF_i0Z-BK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI converts our prompt into embedding form and then goes to the Embedded database to search our required information. For understanding we divide the process of Embedding in for steps. Embedding is a model trained on deeplearning\n",
        "1. We collect the data.\n",
        "2. We categorrise/split the data on the bases of categories i.e. Text, Image, Video, Audio etc.\n",
        "3. We Embed the data in which our data is converted into numbers which machine understands and assigns the suitable numbers on the basis of category and then when we give the prompt the machine converts our prompt in numbers and then matches the numbers of prompt keywords with the stored/embedded data through searching and gives the results. So, in step 3. the categorise data is embedded in numbers with the help of pre-trained Models that embed our data into numbers. So, the most important thing is to select relevant embedding model related to your domain.\n",
        "4. In step 4: the data is stored in Vector Database after embedding  that is used later on when required."
      ],
      "metadata": {
        "id": "uPz-JRqgWUTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Search Formula: Concise Formula**"
      ],
      "metadata": {
        "id": "StH66WXGgkgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector search formula also known as Cousine Formual; it telss angle between two vectors. If the angle is close/nearer that means the relationship or relativity is strong and vice versa. For this purpose https://projector.tensorflow.org/ can be seen for the better understandig about vector search.\n"
      ],
      "metadata": {
        "id": "bLi3hJ1OgtNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rwuX-GHNVSNo",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea192b8-292d-41b0-a4d0-0d4f928007da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/427.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai\n",
        "#-qu means to install latest packages by sending not much messages\n",
        "# secondly we used the rappers of pipecone and google genai API's so that we can call them consistently."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing Pinecone by Adding Pinecone API Key in Colab. In case of local development it is not required. But we are doing on Colab so that not only we can share but also we have all the installation already setup in colab environment. Here ServerlessSpec is a library provided by Pinecone, it makes the database serverless. Pinecone is using cloud aws/azure and when we make a database it asks where to save you database by this library (ServerlessSpec)"
      ],
      "metadata": {
        "id": "fDwjjXFDTrC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "pc = Pinecone(api_key=pinecone_api_key) #here we initialize the object of pinecone pc."
      ],
      "metadata": {
        "id": "unr-xxu2lYTm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Index in Pinecone Vector Database"
      ],
      "metadata": {
        "id": "M0tTGIntdB37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Index in Pinecone Vector Database\n",
        "Dimensions = There are three types of dimensions, 384, 512 and 768 and hese dimensions depeend on accuracy. For example if we ask a question from the LLM it will go in 768 dimensions, 384 or 512 dimension and will match the answer and will give the answer. However, high dimension model will be big one and will give more accuracy, however, we will use more GPUs of our system. Because it have 768 norms/neural networks/contacts/direction, reachability in our provided data to search the answers of our prompt.Fewer than this 512 will give less accuracy results and then 384 will be general nature response from the LLM and it will not go in detail."
      ],
      "metadata": {
        "id": "YE_ZCvYtKwZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"project-2-rag\"\n",
        "pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=768,  # Dimensions of the embeddings\n",
        "            metric=\"cosine\",  # Metric to use for distance calculations\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "        )\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "Gz-KrlojW9VJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding: Here we set the embedding model and make instance of our Embedding class."
      ],
      "metadata": {
        "id": "FxmSq-OzK43V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY') #We get api from google colab through userdata and give it to the system so that it can get whenever required through system environment.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\") # we can search Chtgpt to find related model of embedding"
      ],
      "metadata": {
        "id": "lxtSZHAuKoth"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above class GoogleGenerativeAIEmbeddings we have initialized the class and have created an object embeddings and that embeddings object may have many functions. Below function \"embed_query\" is the function of embeddings. By this function we have embedded the words \"Hello World\" and have presented the embedded data in digits."
      ],
      "metadata": {
        "id": "lCbx4ioFOKHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"hello world\")"
      ],
      "metadata": {
        "id": "zKYR2c0GNfLg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this will print the vectors of above word \"Hello World\""
      ],
      "metadata": {
        "id": "weNHEBesOD0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector[:4]"
      ],
      "metadata": {
        "id": "TvmONGmuO3Oz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "f69d403f-4fb2-4d97-efb4-40ee6c4a5fc4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04909781739115715,\n",
              " -0.044328317046165466,\n",
              " -0.025365281850099564,\n",
              " -0.030721040442585945]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already make index in previous cells, In this process we are rapping index in another layer PineConeVectorStore that is a class of pinecone and whenever we will need we will used this one."
      ],
      "metadata": {
        "id": "mO93dCZ-Tr8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "UFviIqVLTa1n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this process we made 10 documents and put them in an array.\n",
        "UUIDs used to give IDS to our documents. UUID creates random string each time it is called."
      ],
      "metadata": {
        "id": "W8OpcM3cUHBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Save\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie according to the requirements of the weather. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "]"
      ],
      "metadata": {
        "id": "GeqaMdLMT1y5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RsjfT8bVLZ3",
        "outputId": "2ef73b34-74ae-4f98-835f-3ee176ff206f",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are giving unique ID to our all documents and store them in vector store by function add_documents.\n",
        "This method (add documents) uses documents and ids and then it creates embeddings."
      ],
      "metadata": {
        "id": "Y8s8_vHRV17c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "id": "IBXNzdNXVH-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0421b318-ffc8-4cf8-9bf3-eda629c6a322",
        "collapsed": true
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['acac5f33-d419-4d00-a19b-499ce35140e9',\n",
              " 'd0619e43-59c2-4fe4-abd5-333c5376c8e5',\n",
              " 'ae4c782e-1d23-4fe0-b78e-0cc510b84a12',\n",
              " '6149ecd7-61af-4a65-8f21-afee0fe38928',\n",
              " '95af6f26-92dc-4956-b036-946c28e7d42a']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we asked to provide two documents which should have nearby results of the prompt \"Langchain provide.....easy\""
      ],
      "metadata": {
        "id": "npU6rqG4V8ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2, #Specifies that you want to retrieve the top2 most similar documents.\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
        "#This line prints the content (page_content) of each result, along with its associated metadata.\n",
        "#(metadata) which in this case includes the source."
      ],
      "metadata": {
        "id": "DOPrYR9QWCXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4b904f-0d48-40a5-97c9-4387d0629a47"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will get the score of our similarity index."
      ],
      "metadata": {
        "id": "1nvQNGNwY5dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"What about weather?\", k=2, filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "id": "CEtNwitVXnHX",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b1fe9c-a7c4-4722-fc54-b6b819590653"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.622348] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
            "* [SIM=0.622348] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "bpXrI3OxvM48"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_to_user(query: str):\n",
        "        #Vector Search\n",
        "    vector_results = vector_store.similarity_search(query, k=2)\n",
        "    print(len(vector_results))\n",
        "    final_answer = llm.invoke(f\"ANSWER THIS USER QUERY: {query}, Here are some reference to answer {vector_results}\")\n",
        "    return final_answer\n",
        ""
      ],
      "metadata": {
        "id": "q3t6nDb8wOcY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = answer_to_user(\"What is discussed about weather?\")\n",
        "print(answer.content)"
      ],
      "metadata": {
        "id": "OVTqHAz1SzgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3efee64-bc13-4c13-a4e9-bdf092db2aff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "The provided text mentions weather in two different ways.  One mentions a movie being \"amazing according to the requirements of the weather,\" which is vague and doesn't specify what weather conditions were involved. The other provides a specific weather forecast: cloudy and overcast with a high of 62 degrees.\n"
          ]
        }
      ]
    }
  ]
}